#!/usr/bin/env python3
"""
æ’åºç®—æ³•æ€§èƒ½åˆ†æä¸å¯è§†åŒ–
æ”¯æŒå¤šä¼˜åŒ–çº§åˆ«æµ‹è¯•ã€æ—¶é—´å¤æ‚åº¦åˆ†æå’ŒçŸ¢é‡å›¾ç”Ÿæˆ
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from scipy.optimize import curve_fit
import os
from datetime import datetime

# è®¾ç½®å›¾è¡¨é£æ ¼
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

class SortingPerformanceAnalyzer:
    def __init__(self):
        self.df = None
        self.theoretical_complexity = {
            'QuickSort_Recursive': 'O(n log n)',
            'QuickSort_NonRecursive': 'O(n log n)',
            'MergeSort_Sequential': 'O(n log n)',
            'MergeSort_Parallel': 'O(n log n)'
        }
        
    def load_data(self, filename='../results/performance_data.csv'):
        """åŠ è½½æ€§èƒ½æ•°æ®"""
        try:
            self.df = pd.read_csv(filename)
            print("âœ… æ•°æ®åŠ è½½æˆåŠŸ!")
            print(f"æ•°æ®è§„æ¨¡: {len(self.df)} æ¡è®°å½•")
            print(f"ä¼˜åŒ–çº§åˆ«: {self.df['Optimization'].unique()}")
            print(f"ç®—æ³•: {self.df['Algorithm'].unique()}")
            print(f"æ•°æ®è§„æ¨¡èŒƒå›´: {self.df['DataSize'].min()} - {self.df['DataSize'].max()}")
            return True
        except FileNotFoundError:
            print("âŒ æ€§èƒ½æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œæµ‹è¯•ç¨‹åº")
            return False
    
    def preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†"""
        if self.df is None:
            return
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        self.df['TimePerElement'] = self.df['Time'] / self.df['DataSize']
        self.df['ComparisonsPerElement'] = self.df['Comparisons'] / self.df['DataSize']
        self.df['SwapsPerElement'] = self.df['Swaps'] / self.df['DataSize']
        self.df['MemoryPerElement'] = self.df['MemoryUsage'] / self.df['DataSize']
        
        # æ·»åŠ ç®—æ³•ç±»å‹åˆ†ç±»
        self.df['AlgorithmType'] = self.df['Algorithm'].apply(
            lambda x: 'å¿«é€Ÿæ’åº' if 'Quick' in x else 'å½’å¹¶æ’åº'
        )
        
        print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ!")
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
        if self.df is None:
            return
        
        print("\n" + "="*80)
        print("                    æ’åºç®—æ³•æ€§èƒ½åˆ†ææŠ¥å‘Š")
        print("="*80)
        print(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æ•°æ®è®°å½•æ€»æ•°: {len(self.df)}")
        
        # æœ€ä½³æ€§èƒ½åˆ†æ
        best_performance = self.df.loc[self.df.groupby(['DataSize', 'Algorithm'])['Time'].idxmin()]
        
        print("\nğŸ† å„ç®—æ³•æœ€ä½³ä¼˜åŒ–çº§åˆ«:")
        summary_table = best_performance[['DataSize', 'Algorithm', 'Optimization', 'Time', 'Comparisons', 'MemoryUsage']].copy()
        summary_table['Time'] = summary_table['Time'].round(6)
        summary_table['MemoryUsage_MB'] = (summary_table['MemoryUsage'] / 1024 / 1024).round(2)
        print(summary_table.to_string(index=False))
        
        # æ€§èƒ½æå‡åˆ†æ
        print("\nğŸ“ˆ ä¼˜åŒ–çº§åˆ«æ€§èƒ½æå‡åˆ†æ (ç›¸å¯¹äº-O0):")
        optimization_levels = ['O1', 'O2', 'O3', 'Ofast']
        
        for size in sorted(self.df['DataSize'].unique()):
            print(f"\næ•°æ®è§„æ¨¡ {size:,}:")
            size_data = self.df[self.df['DataSize'] == size]
            o0_data = size_data[size_data['Optimization'] == 'O0']
            
            for algo in self.df['Algorithm'].unique():
                o0_time = o0_data[o0_data['Algorithm'] == algo]['Time'].mean()
                if not np.isnan(o0_time):
                    improvements = []
                    for opt in optimization_levels:
                        opt_time = size_data[
                            (size_data['Algorithm'] == algo) & 
                            (size_data['Optimization'] == opt)
                        ]['Time'].mean()
                        if not np.isnan(opt_time):
                            improvement = ((o0_time - opt_time) / o0_time) * 100
                            improvements.append(f"{opt}: {improvement:+.1f}%")
                    
                    if improvements:
                        print(f"  {algo:<25} {', '.join(improvements)}")
    
    def plot_optimization_impact(self):
        """ç»˜åˆ¶ä¼˜åŒ–çº§åˆ«å½±å“åˆ†æ"""
        if self.df is None:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('ç¼–è¯‘ä¼˜åŒ–çº§åˆ«å¯¹æ’åºç®—æ³•æ€§èƒ½çš„å½±å“åˆ†æ', fontsize=14, fontweight='bold')
        
        algorithms = self.df['Algorithm'].unique()
        optimizations = self.df['Optimization'].unique()
        
        # 1. æ‰§è¡Œæ—¶é—´å¯¹æ¯”
        for i, algo in enumerate(algorithms):
            ax = axes[i//2, i%2]
            algo_data = self.df[self.df['Algorithm'] == algo]
            
            for opt in optimizations:
                opt_data = algo_data[algo_data['Optimization'] == opt]
                if not opt_data.empty:
                    grouped = opt_data.groupby('DataSize')['Time'].mean()
                    ax.plot(grouped.index, grouped.values, 
                           marker='o', linewidth=2, label=opt, markersize=4)
            
            ax.set_title(f'{algo}', fontweight='bold')
            ax.set_xlabel('æ•°æ®è§„æ¨¡')
            ax.set_ylabel('æ‰§è¡Œæ—¶é—´ (ç§’)')
            ax.set_xscale('log')
            ax.set_yscale('log')
            ax.legend(title='ä¼˜åŒ–çº§åˆ«', fontsize=8)
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('../results/optimization_impact.pdf', bbox_inches='tight', dpi=300)
        plt.savefig('../results/optimization_impact.png', bbox_inches='tight', dpi=300)
        plt.show()
    
    def plot_algorithm_comparison(self):
        """ç»˜åˆ¶ç®—æ³•æ€§èƒ½å¯¹æ¯”"""
        if self.df is None:
            return
        
        # ä½¿ç”¨O2ä¼˜åŒ–çº§åˆ«çš„æ•°æ®è¿›è¡Œæ¯”è¾ƒ
        o2_data = self.df[self.df['Optimization'] == 'O2']
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('æ’åºç®—æ³•æ€§èƒ½å¯¹æ¯”åˆ†æ (O2ä¼˜åŒ–çº§åˆ«)', fontsize=14, fontweight='bold')
        
        metrics = ['Time', 'Comparisons', 'Swaps', 'MemoryUsage']
        metric_names = ['æ‰§è¡Œæ—¶é—´ (ç§’)', 'æ¯”è¾ƒæ¬¡æ•°', 'äº¤æ¢æ¬¡æ•°', 'å†…å­˜ä½¿ç”¨ (å­—èŠ‚)']
        scales = ['log', 'log', 'log', 'log']
        
        for idx, (metric, name, scale) in enumerate(zip(metrics, metric_names, scales)):
            ax = axes[idx//2, idx%2]
            
            for algo in o2_data['Algorithm'].unique():
                algo_metric_data = o2_data[o2_data['Algorithm'] == algo]
                if not algo_metric_data.empty:
                    grouped = algo_metric_data.groupby('DataSize')[metric].mean()
                    ax.plot(grouped.index, grouped.values, 
                           marker='s', linewidth=2, label=algo, markersize=4)
            
            ax.set_title(name)
            ax.set_xlabel('æ•°æ®è§„æ¨¡')
            ax.set_ylabel(name)
            ax.set_xscale('log')
            if scale == 'log':
                ax.set_yscale('log')
            ax.legend(fontsize=8)
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('../results/algorithm_comparison.pdf', bbox_inches='tight', dpi=300)
        plt.savefig('../results/algorithm_comparison.png', bbox_inches='tight', dpi=300)
        plt.show()
    
    def theoretical_complexity_analysis(self):
        """ç†è®ºæ—¶é—´å¤æ‚åº¦åˆ†æ"""
        if self.df is None:
            return
        
        # ç†è®ºå¤æ‚åº¦å‡½æ•°
        def n_log_n(x, a, b):
            return a * x * np.log(x) + b
        
        def n_squared(x, a, b):
            return a * x * x + b
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('æ’åºç®—æ³•æ—¶é—´å¤æ‚åº¦éªŒè¯åˆ†æ', fontsize=14, fontweight='bold')
        
        algorithms = self.df['Algorithm'].unique()
        o2_data = self.df[self.df['Optimization'] == 'O2']
        
        for i, algo in enumerate(algorithms):
            ax = axes[i//2, i%2]
            algo_data = o2_data[o2_data['Algorithm'] == algo]
            
            if len(algo_data) < 3:
                continue
            
            # å®é™…æµ‹é‡æ•°æ®
            sizes = algo_data['DataSize'].unique()
            times = []
            
            for size in sizes:
                time_val = algo_data[algo_data['DataSize'] == size]['Time'].mean()
                times.append(time_val)
            
            sizes = np.array(sizes)
            times = np.array(times)
            
            # ç»˜åˆ¶å®é™…æ•°æ®ç‚¹
            ax.scatter(sizes, times, color='red', s=50, zorder=5, 
                      label='å®æµ‹æ•°æ®', alpha=0.7)
            
            # å°è¯•ä¸åŒå¤æ‚åº¦æ¨¡å‹çš„æ‹Ÿåˆ
            try:
                # n log n æ‹Ÿåˆ
                popt_log, pcov_log = curve_fit(n_log_n, sizes, times, 
                                             p0=[1e-7, 0], maxfev=5000)
                sizes_fit = np.linspace(min(sizes), max(sizes), 100)
                times_fit_log = n_log_n(sizes_fit, *popt_log)
                
                # è®¡ç®—RÂ²
                residuals_log = times - n_log_n(sizes, *popt_log)
                ss_res_log = np.sum(residuals_log**2)
                ss_tot_log = np.sum((times - np.mean(times))**2)
                r_squared_log = 1 - (ss_res_log / ss_tot_log)
                
                ax.plot(sizes_fit, times_fit_log, 'b-', linewidth=2,
                       label=f'n log n æ‹Ÿåˆ (RÂ² = {r_squared_log:.4f})')
                
                # å°è¯•nÂ²æ‹Ÿåˆå¯¹æ¯”
                try:
                    popt_sq, pcov_sq = curve_fit(n_squared, sizes, times, 
                                               p0=[1e-9, 0], maxfev=5000)
                    times_fit_sq = n_squared(sizes_fit, *popt_sq)
                    
                    residuals_sq = times - n_squared(sizes, *popt_sq)
                    ss_res_sq = np.sum(residuals_sq**2)
                    r_squared_sq = 1 - (ss_res_sq / ss_tot_log)
                    
                    ax.plot(sizes_fit, times_fit_sq, 'g--', linewidth=1,
                           label=f'nÂ² æ‹Ÿåˆ (RÂ² = {r_squared_sq:.4f})', alpha=0.7)
                except:
                    pass
                
            except Exception as e:
                print(f"æ‹Ÿåˆå¤±è´¥ {algo}: {e}")
                # ç›´æ¥ç»˜åˆ¶è¿çº¿
                ax.plot(sizes, times, 'b-', linewidth=1, label='å®æµ‹è¶‹åŠ¿')
            
            ax.set_title(f'{algo}\nç†è®ºå¤æ‚åº¦: {self.theoretical_complexity[algo]}', 
                        fontweight='bold')
            ax.set_xlabel('æ•°æ®è§„æ¨¡ n')
            ax.set_ylabel('æ‰§è¡Œæ—¶é—´ (ç§’)')
            ax.set_xscale('log')
            ax.set_yscale('log')
            ax.legend(fontsize=8)
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('../results/complexity_analysis.pdf', bbox_inches='tight', dpi=300)
        plt.savefig('../results/complexity_analysis.png', bbox_inches='tight', dpi=300)
        plt.show()
    
    def plot_parallel_efficiency(self):
        """å¹¶è¡Œæ•ˆç‡åˆ†æ"""
        if self.df is None:
            return
        
        # åˆ†æå¹¶è¡Œå½’å¹¶æ’åºçš„æ•ˆç‡
        parallel_data = self.df[self.df['Algorithm'] == 'MergeSort_Parallel']
        sequential_data = self.df[self.df['Algorithm'] == 'MergeSort_Sequential']
        
        if parallel_data.empty or sequential_data.empty:
            print("ç¼ºå°‘å¹¶è¡Œ/é¡ºåºå½’å¹¶æ’åºæ•°æ®")
            return
        
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        fig.suptitle('å¹¶è¡Œå½’å¹¶æ’åºæ•ˆç‡åˆ†æ', fontsize=14, fontweight='bold')
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        speedup_data = []
        for size in sequential_data['DataSize'].unique():
            seq_time = sequential_data[
                (sequential_data['DataSize'] == size) & 
                (sequential_data['Optimization'] == 'O2')
            ]['Time'].mean()
            
            par_time = parallel_data[
                (parallel_data['DataSize'] == size) & 
                (parallel_data['Optimization'] == 'O2')
            ]['Time'].mean()
            
            if not np.isnan(seq_time) and not np.isnan(par_time) and par_time > 0:
                speedup = seq_time / par_time
                speedup_data.append((size, speedup))
        
        if speedup_data:
            sizes, speedups = zip(*speedup_data)
            
            # åŠ é€Ÿæ¯”
            axes[0].plot(sizes, speedups, 'bo-', linewidth=2, markersize=6)
            axes[0].axhline(y=1, color='r', linestyle='--', alpha=0.7, label='åŸºçº¿')
            axes[0].set_xlabel('æ•°æ®è§„æ¨¡')
            axes[0].set_ylabel('åŠ é€Ÿæ¯” (é¡ºåºæ—¶é—´/å¹¶è¡Œæ—¶é—´)')
            axes[0].set_xscale('log')
            axes[0].set_title('å¹¶è¡ŒåŠ é€Ÿæ¯”åˆ†æ')
            axes[0].legend()
            axes[0].grid(True, alpha=0.3)
            
            # æ•ˆç‡åˆ†æ
            # å‡è®¾ä½¿ç”¨4ä¸ªçº¿ç¨‹ï¼ˆæ ¹æ®OpenMPé»˜è®¤è®¾ç½®ï¼‰
            theoretical_threads = 4
            efficiencies = [s / theoretical_threads for s in speedups]
            axes[1].plot(sizes, efficiencies, 'go-', linewidth=2, markersize=6)
            axes[1].axhline(y=1, color='r', linestyle='--', alpha=0.7, label='ç†æƒ³æ•ˆç‡')
            axes[1].set_xlabel('æ•°æ®è§„æ¨¡')
            axes[1].set_ylabel('å¹¶è¡Œæ•ˆç‡')
            axes[1].set_xscale('log')
            axes[1].set_title('å¹¶è¡Œæ•ˆç‡åˆ†æ')
            axes[1].legend()
            axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('../results/parallel_efficiency.pdf', bbox_inches='tight', dpi=300)
        plt.savefig('../results/parallel_efficiency.png', bbox_inches='tight', dpi=300)
        plt.show()
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        if self.df is None:
            return
        
        # ä¿å­˜è¯¦ç»†æ•°æ®åˆ°Excel
        with pd.ExcelWriter('../results/sorting_performance_analysis.xlsx') as writer:
            # åŸå§‹æ•°æ®
            self.df.to_excel(writer, sheet_name='åŸå§‹æ•°æ®', index=False)
            
            # æ±‡æ€»ç»Ÿè®¡
            summary = self.df.groupby(['Algorithm', 'Optimization', 'DataSize']).agg({
                'Time': ['mean', 'std', 'min', 'max'],
                'Comparisons': 'mean',
                'Swaps': 'mean',
                'MemoryUsage': 'mean'
            }).round(6)
            summary.to_excel(writer, sheet_name='æ±‡æ€»ç»Ÿè®¡')
            
            # æœ€ä½³æ€§èƒ½
            best_performance = self.df.loc[self.df.groupby(['DataSize', 'Algorithm'])['Time'].idxmin()]
            best_performance.to_excel(writer, sheet_name='æœ€ä½³æ€§èƒ½', index=False)
        
        print("âœ… è¯¦ç»†åˆ†ææŠ¥å‘Šå·²ä¿å­˜ä¸º '../results/sorting_performance_analysis.xlsx'")
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        if not self.load_data():
            return
        
        self.preprocess_data()
        self.generate_summary_report()
        self.plot_optimization_impact()
        self.plot_algorithm_comparison()
        self.theoretical_complexity_analysis()
        self.plot_parallel_efficiency()
        self.generate_comprehensive_report()
        
        print("\nğŸ‰ åˆ†æå®Œæˆ! ç”Ÿæˆçš„æ–‡ä»¶:")
        print("ğŸ“Š å›¾è¡¨æ–‡ä»¶:")
        print("   - optimization_impact.pdf/png (ä¼˜åŒ–çº§åˆ«å½±å“)")
        print("   - algorithm_comparison.pdf/png (ç®—æ³•å¯¹æ¯”)")
        print("   - complexity_analysis.pdf/png (å¤æ‚åº¦åˆ†æ)")
        print("   - parallel_efficiency.pdf/png (å¹¶è¡Œæ•ˆç‡)")
        print("ğŸ“ˆ æ•°æ®æ–‡ä»¶:")
        print("   - sorting_performance_analysis.xlsx (å®Œæ•´æ•°æ®åˆ†æ)")
        print("   - performance_data.csv (åŸå§‹æ•°æ®)")

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("       æ’åºç®—æ³•æ€§èƒ½æ•°æ®åˆ†æä¸å¯è§†åŒ–ç³»ç»Ÿ")
    print("="*60)
    
    analyzer = SortingPerformanceAnalyzer()
    analyzer.run_complete_analysis()

if __name__ == "__main__":
    main()
